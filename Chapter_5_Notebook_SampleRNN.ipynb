{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gk3nTyU-eEvj"
      },
      "source": [
        "\n",
        "\n",
        "This notebook provides the means to:\n",
        "\n",
        "- download a solo saxophone dataset from the repository provided.\n",
        "- augment and chunk the data to make it suitable for training a SampleRNN model on.\n",
        "- train a SampleRNN model.\n",
        "- generate fake saxophone playing from the trained model.\n",
        "\n",
        "All that is required is a Google Drive account and a HuggingFace account. You can use your Google account to create a HuggingFace account. Cell 4 below requires you to generate and paste an access token https://huggingface.co/settings/tokens\n",
        "\n",
        "Before running any of the cells below, go to Edit -> Notebook Settings above and select the \"High RAM\" option, otherwise the notebook will crash. Also make sure you have a GPU selected (T4 is fine, A100 much faster if available).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A70pjblWtMC_"
      },
      "outputs": [],
      "source": [
        "# 1 CONNECT TO YOUR GOOGLE DRIVE\n",
        "\n",
        "import os, sys\n",
        "from google.colab import drive\n",
        "drive.mount('/content/mnt', force_remount=True)\n",
        "nb_path = '/content/notebooks'\n",
        "os.symlink('/content/mnt/My Drive/', nb_path)\n",
        "sys.path.insert(0, nb_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1boQFaDtOrm"
      },
      "outputs": [],
      "source": [
        "# 2 CLONE SAMPLERNN REPO\n",
        "\n",
        "%cd {nb_path}\n",
        "!mkdir SampleRNN\n",
        "%cd SampleRNN\n",
        "!git clone https://github.com/rncm-prism/prism-samplernn\n",
        "%cd prism-samplernn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "diLNMrModZG2"
      },
      "outputs": [],
      "source": [
        "# 3 INSTALL HUGGING FACE DATASETS DEPENDENCY\n",
        "\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pnrz7ruYthko"
      },
      "outputs": [],
      "source": [
        "# 4 DOWNLOAD DATASET FROM HUGGINGFACE\n",
        "from datasets import load_dataset\n",
        "\n",
        "# PASTE YOUR HUGGING FACE ACCESS TOKEN HERE, GENERATE ONE FROM YOUR ACCOUNT AT https://huggingface.co/settings/tokens\n",
        "MY_ACCESS_TOKEN = \"\"\n",
        "\n",
        "dataset = load_dataset(\"markhanslip/markhanslip_phd_saxophone_data\", token = MY_ACCESS_TOKEN)\n",
        "\n",
        "data = dataset['train']['audio'][0]['array']\n",
        "sr = dataset['train']['audio'][0]['sampling_rate']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFjdqUU3trqk"
      },
      "outputs": [],
      "source": [
        "# 5 PREPARE DATASET FOR TRAINING (TAKES A WHILE)\n",
        "\n",
        "import soundfile as sf\n",
        "import numpy as np\n",
        "import os\n",
        "import librosa\n",
        "\n",
        "stretched_data = librosa.effects.time_stretch(data, rate=1.05)\n",
        "data = np.hstack((data, stretched_data))\n",
        "\n",
        "data_inv = -data\n",
        "\n",
        "chunk_len = int(sr*8) # 8 second chunks\n",
        "startpos = 0\n",
        "endpos = chunk_len\n",
        "count=0\n",
        "out_dir='./ToneRows_dataset/'\n",
        "\n",
        "if not os.path.exists(out_dir):\n",
        "    os.mkdir(out_dir)\n",
        "\n",
        "for i in range(len(data)):\n",
        "    if i % chunk_len == 0:\n",
        "        count+=1\n",
        "        sf.write(os.path.join(out_dir,'{}.wav'.format(str(count).zfill(6))), samplerate=sr, data=data[startpos:endpos], subtype='PCM_16')\n",
        "        startpos = (startpos+chunk_len)\n",
        "        endpos = (endpos+(chunk_len))\n",
        "\n",
        "startpos = 0\n",
        "endpos = chunk_len\n",
        "count=0\n",
        "\n",
        "for i in range(len(data_inv)):\n",
        "    if i % chunk_len == 0:\n",
        "        count+=1\n",
        "        sf.write(os.path.join(out_dir,'{}_inv.wav'.format(str(count).zfill(6))), samplerate=sr, data=data_inv[startpos:endpos], subtype='PCM_16')\n",
        "        startpos = (startpos+chunk_len)\n",
        "        endpos = (endpos+(chunk_len))\n",
        "\n",
        "files = os.listdir(out_dir)\n",
        "for wavfile in files:\n",
        "  y, sr = sf.read(os.path.join(out_dir, wavfile))\n",
        "  if len(y) != chunk_len:\n",
        "    os.remove(os.path.join(out_dir, wavfile))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16TYjYhovZyS"
      },
      "outputs": [],
      "source": [
        "# 5 create config file for defining the model architecture\n",
        "\n",
        "import json\n",
        "\n",
        "config = {\n",
        "\n",
        "    \"seq_len\": 1024,\n",
        "    \"frame_sizes\": [16,64],\n",
        "    \"dim\": 1024,\n",
        "    \"rnn_type\": \"lstm\",\n",
        "    \"num_rnn_layers\": 3,\n",
        "    \"q_type\": \"mu-law\",\n",
        "    \"q_levels\": 256,\n",
        "    \"emb_size\": 256,\n",
        "    \"skip_conn\": False\n",
        "\n",
        "}\n",
        "\n",
        "with open('./3l_lstm.config.json', 'w') as outfile:\n",
        "    json.dump(config, outfile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zbRt-EdCvGm6"
      },
      "outputs": [],
      "source": [
        "# 6 TRAIN THE MODEL (TAKES A FEW HOURS)\n",
        "!python ./train.py --data_dir ./ToneRows_dataset/ --config_file ./3l_lstm.config.json --output_dir ./ToneRows_training --id ToneRows_training --output_file_dur 3 --batch_size 64 --sample_rate {sr} --num_epochs 120  # if training stops then add --resume True and run again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmxhQtEny3CT"
      },
      "outputs": [],
      "source": [
        "# 7 POST-TRAINING - PICK THE MOST RECENT SAVED CHECKPOINT FOLDER\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import interact, interactive, fixed, interact_manual\n",
        "import os\n",
        "\n",
        "ckpt_dirs=[]\n",
        "\n",
        "for dir in os.listdir('./logdir/ToneRows_training'):\n",
        "  ckpt_dirs.append(dir)\n",
        "\n",
        "def f(x):\n",
        "    return x\n",
        "\n",
        "w=interactive(f, x=widgets.Dropdown(options=ckpt_dirs, description='ckpt dir: ',disabled=False));\n",
        "display(w)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebhi4hyRzFjn"
      },
      "outputs": [],
      "source": [
        "ckpt_dir = w.result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7YUi7rYzGCZ"
      },
      "outputs": [],
      "source": [
        "# 8 - CHOOSE A CHECKPOINT FROM WHICH TO GENERATE\n",
        "\n",
        "from functools import reduce\n",
        "\n",
        "ckpt_files=[]\n",
        "\n",
        "for ckpt in os.listdir(os.path.join('./logdir/ToneRows_training', ckpt_dir)):\n",
        "  if ckpt[:5] == \"model\":\n",
        "    ckpt_files.append(ckpt[11:14])\n",
        "\n",
        "for index, ckpt in enumerate(ckpt_files):\n",
        "  if ckpt[-1] == \"i\" or ckpt[-1] == \"d\":\n",
        "    ckpt_files[index] = ckpt[0]\n",
        "  elif ckpt[-1] == \".\":\n",
        "    ckpt_files[index] = ckpt[:2]\n",
        "  else:\n",
        "    pass\n",
        "\n",
        "unique_ckpts = reduce(lambda l, x: l.append(x) or l if x not in l else l, ckpt_files, [])\n",
        "\n",
        "def f(x):\n",
        "    return x\n",
        "\n",
        "v=interactive(f, x=widgets.Dropdown(options=unique_ckpts, description='ckpt: ',disabled=False));\n",
        "display(v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ina3c5MtzTzG"
      },
      "outputs": [],
      "source": [
        "ckpt = v.result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikvIJuZhzUKs"
      },
      "outputs": [],
      "source": [
        "# 9 - GENERATE SAMPLES\n",
        "import os\n",
        "\n",
        "NUM_FILES = 5 # number of audio files to generate, can be any int\n",
        "FILE_LENGTH = 8 # length per generated file in seconds, can be any int (very long samples will take a long time to generate)\n",
        "OUTPUT_PATH = \"ToneRows_samples\"\n",
        "FILE_PREFIX = \"ToneRows\"\n",
        "\n",
        "if not os.path.exists(OUTPUT_PATH):\n",
        "  os.mkdir(OUTPUT_PATH)\n",
        "\n",
        "!python ./generate.py --output_path ./{OUTPUT_PATH}/{FILE_PREFIX} --checkpoint_path ./logdir/ToneRows_training/{ckpt_dir}/model.ckpt-{ckpt} --config_file ./3l_lstm.config.json --dur {FILE_LENGTH} --num_seqs {NUM_FILES} --sample_rate 22050 --temperature 0.95"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aI6acdYeYmmJ"
      },
      "outputs": [],
      "source": [
        "# 10 - LISTEN BACK TO GENERATED SAMPLES\n",
        "\n",
        "import soundfile\n",
        "from IPython.display import Audio, display\n",
        "import time\n",
        "import random\n",
        "import os\n",
        "\n",
        "wavfiles = os.listdir(OUTPUT_PATH)\n",
        "\n",
        "for wavfile in wavfiles:\n",
        "  y, sr = soundfile.read(os.path.join(OUTPUT_PATH, wavfile))\n",
        "  widget = Audio(y, rate=sr, autoplay=False)\n",
        "  display(widget)\n",
        "  time.sleep(len(y)/sr + random.choice([0.1, 0.05, 0.2, 0.15, 0.25]))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
